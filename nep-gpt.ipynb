{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7609791,"sourceType":"datasetVersion","datasetId":4430960}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install git+https://github.com/huggingface/transformers datasets\n!pip install sentencepiece\n!pip install accelerate -U","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-14T07:48:52.135739Z","iopub.execute_input":"2024-02-14T07:48:52.136100Z","iopub.status.idle":"2024-02-14T07:50:04.691140Z","shell.execute_reply.started":"2024-02-14T07:48:52.136064Z","shell.execute_reply":"2024-02-14T07:50:04.690065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tokenizers.models import BPE\nfrom tokenizers import Tokenizer\nfrom tokenizers.decoders import ByteLevel as ByteLevelDecoder\nfrom tokenizers.normalizers import NFKC, Sequence\nfrom tokenizers.pre_tokenizers import ByteLevel\nfrom tokenizers.trainers import BpeTrainer\nfrom dataclasses import dataclass\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:50:04.693060Z","iopub.execute_input":"2024-02-14T07:50:04.693390Z","iopub.status.idle":"2024-02-14T07:50:04.739930Z","shell.execute_reply.started":"2024-02-14T07:50:04.693360Z","shell.execute_reply":"2024-02-14T07:50:04.739255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tokenizers import ByteLevelBPETokenizer\nfrom pathlib import Path\nimport os\n\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train(files='nepali-wikipedia/output.txt', vocab_size=52_000, min_frequency=2, special_tokens=[\n    \"<s>\",\n    \"<pad>\",\n    \"</s>\",\n    \"<unk>\",\n    \"<mask>\",\n])\n\nsave_path = 'nep-gpt'\nif not os.path.exists(save_path):\n      os.makedirs(save_path)\ntokenizer.save_model(save_path)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:50:04.740852Z","iopub.execute_input":"2024-02-14T07:50:04.741085Z","iopub.status.idle":"2024-02-14T07:50:26.222979Z","shell.execute_reply.started":"2024-02-14T07:50:04.741064Z","shell.execute_reply":"2024-02-14T07:50:26.222112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport torch\nimport math\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\n\ndef gelu(x):\n  return 0.5*x*(1+torch.tanh(math.sqrt(2/math.pi)*(x+0.044715*torch.pow(x,3))))\n\nclass LayerNorm(nn.Module):\n  def __init__(self, hidden_size, eps=1e-12):\n    super(LayerNorm, self).__init__()\n    self.weight = nn.Parameter(torch.ones(hidden_size))\n    self.bias = nn.Parameter(torch.zeros(hidden_size))\n    self.variance_epsilon = eps\n\n  def forward(self, x):\n    u = x.mean(-1, keepdim=True)\n    s = (x - u).pow(2).mean(-1, keepdim=True)\n    x = (x - u) / torch.sqrt(s + self.variance_epsilon)  # very small value is added to avoid division by zero error if it happens\n    return self.weight * x + self.bias  # w*x+b","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:50:26.225221Z","iopub.execute_input":"2024-02-14T07:50:26.225500Z","iopub.status.idle":"2024-02-14T07:50:27.926267Z","shell.execute_reply.started":"2024-02-14T07:50:26.225476Z","shell.execute_reply":"2024-02-14T07:50:27.925443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv1D(nn.Module):\n  def __init__(self,nf,nx):\n    super(Conv1D,self).__init__()\n    self.nf = nf\n    w = torch.empty(nx,nf)\n    nn.init.normal_(w,std=0.02)\n    self.weight = Parameter(w)\n    self.bias = Parameter(torch.zeros(nf))\n\n  def forward(self,x):\n    size_out = x.size()[:-1] + (self.nf,) # Prepare for matrix multiplication\n    x = torch.addmm(self.bias, x.view(-1,x.size(-1)),self.weight)\n    x = x.view(*size_out)\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:50:27.927390Z","iopub.execute_input":"2024-02-14T07:50:27.927768Z","iopub.status.idle":"2024-02-14T07:50:27.934641Z","shell.execute_reply.started":"2024-02-14T07:50:27.927741Z","shell.execute_reply":"2024-02-14T07:50:27.933819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx\n        assert n_state % config.n_head == 0\n        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state*3, nx)\n        self.c_proj = Conv1D(n_state, nx)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        nd, ns = w.size(-2), w.size(-1)\n        b = self.bias[:, :, ns - nd:ns, :ns]\n        w = w * b - 1e10 * (1 - b)  # subtract large negative where bias is 0\n        w = nn.Softmax(dim=-1)(w)\n        return torch.matmul(w, v)\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)\n        if k:\n            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n        else:\n            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n\n    def forward(self, x, layer_past=None):\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n\n        if layer_past is not None:\n            past_key, past_value = layer_past[0].transpose(-2,-1), layer_past[1]\n            key = torch.cat((past_value,key), dim=-1)\n            value = torch.cat((past_value,value), dim=-2)\n\n        present = torch.stack((key.transpose(-2,-1), value))  # transpose to have same shape for stacking\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n\n        return a, present\n","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:50:27.935873Z","iopub.execute_input":"2024-02-14T07:50:27.936155Z","iopub.status.idle":"2024-02-14T07:50:27.952640Z","shell.execute_reply.started":"2024-02-14T07:50:27.936107Z","shell.execute_reply":"2024-02-14T07:50:27.951789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, n_state, config):\n        super(MLP, self).__init__()\n        nx = config.n_embd\n        self.c_fc = Conv1D(n_state, nx)\n        self.c_proj = Conv1D(nx, n_state)\n        self.act = gelu\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return h2\n\n\nclass Block(nn.Module):\n  def __init__(self,n_ctx,config,scale=False):\n    super(Block,self).__init__()\n    nx = config.n_embd\n    self.ln_1 = LayerNorm(nx,eps=config.layer_norm_epsilon)\n    self.attn = Attention(nx,n_ctx,config,scale)\n    self.ln_2 = LayerNorm(nx,eps=config.layer_norm_epsilon)\n    self.mlp = MLP(4*nx,config)\n\n  def forward(self,x,layer_past=None):\n    a,present = self.attn(self.ln_1(x),layer_past=layer_past)\n    x = x+a\n    m = self.mlp(self.ln_2(x))\n    x = x + m\n    return x, present\nclass GPT2Model(nn.Module):\n    def __init__(self, config):\n        super(GPT2Model, self).__init__()\n        self.n_layer = config.n_layer\n        self.n_embd = config.n_embd\n        self.n_vocab = config.vocab_size\n\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n        block = Block(config.n_ctx, config, scale=True)\n        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n\n    def set_embeddings_weights(self, model_embeddings_weights):\n        embed_shape = model_embeddings_weights.shape\n        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n        self.decoder.weight = model_embeddings_weights  # tied weights\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None):\n        if past is None:\n            past_length = 0\n            past = [None] * len(self.h)\n        else:\n            past_length = past[0][0].size(-2)\n\n        if position_ids is None:\n            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        position_ids = position_ids.view(-1, position_ids.size(-1))\n        input_embeds = self.wte(input_ids)\n        position_embeds = self.wpe(position_ids)\n\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n            token_type_embeds = self.wte(token_type_ids)\n        else:\n            token_type_embeds = torch.zeros_like(input_embeds)\n\n        hidden_states = input_embeds + position_embeds + token_type_embeds\n        presents = list()\n\n        for block, layer_past in zip(self.h, past):\n            hidden_states, present = block(hidden_states, layer_past)\n            presents.append(present)\n\n        hidden_states = self.ln_f(hidden_states)\n        output_shape = input_shape + (hidden_states.size(-1),)\n\n        return hidden_states.view(*output_shape), presents","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:50:27.953604Z","iopub.execute_input":"2024-02-14T07:50:27.953881Z","iopub.status.idle":"2024-02-14T07:50:27.973015Z","shell.execute_reply.started":"2024-02-14T07:50:27.953858Z","shell.execute_reply":"2024-02-14T07:50:27.972150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GPT2LMHead(nn.Module):\n  def __init__(self, model_embeddings_weights, config):\n    super(GPT2LMHead, self).__init__()\n    self.n_embd = config.n_embd\n    self.set_embeddings_weights(model_embeddings_weights)\n\n  def set_embeddings_weights(self, model_embeddings_weights):\n    embed_shape = model_embeddings_weights.shape\n    self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n    self.decoder.weight = model_embeddings_weights  # tied weights\n\n  def forward(self, hidden_state):\n    lm_logits = self.decoder(hidden_state)\n    return lm_logits\n\nclass GPT2LMHeadModel(nn.Module):\n    def __init__(self, config):\n        super(GPT2LMHeadModel, self).__init__()\n        self.transformer = GPT2Model(config)\n        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n\n    def set_tied(self):\n        \"\"\" Make sure we are sharing the embeddings\n        \"\"\"\n        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        lm_logits = self.lm_head(hidden_states)\n        if lm_labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\n            return loss\n        return lm_logits, presents","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:50:27.974278Z","iopub.execute_input":"2024-02-14T07:50:27.974655Z","iopub.status.idle":"2024-02-14T07:50:27.986797Z","shell.execute_reply.started":"2024-02-14T07:50:27.974625Z","shell.execute_reply":"2024-02-14T07:50:27.986038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Config, GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained(save_path)\n\ntokenizer.add_special_tokens({\n    \"eos_token\":\"</s>\",\n    \"bos_token\":\"<s>\",\n    \"unk_token\":\"<unk>\",\n    \"pad_token\":\"<pad>\",\n    \"mask_token\":\"<mask>\"\n})\n\nconfig = GPT2Config(\n    vocab_size=tokenizer.vocab_size,\n    bos_token_id=tokenizer.bos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n    n_ctx= 1024\n)\n\nmodel = GPT2LMHeadModel(config)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:50:27.987757Z","iopub.execute_input":"2024-02-14T07:50:27.988012Z","iopub.status.idle":"2024-02-14T07:50:31.097053Z","shell.execute_reply.started":"2024-02-14T07:50:27.987991Z","shell.execute_reply":"2024-02-14T07:50:31.096187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LineByLineTextDataset\n\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"/nepali-wikipedia/output.txt\",\n    block_size=128,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:50:31.100448Z","iopub.execute_input":"2024-02-14T07:50:31.100972Z","iopub.status.idle":"2024-02-14T07:53:48.271122Z","shell.execute_reply.started":"2024-02-14T07:50:31.100944Z","shell.execute_reply":"2024-02-14T07:53:48.270260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\ntrain_dataset,test_dataset = train_test_split(dataset,test_size=0.2)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=False)\n\ntrain_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True,collate_fn=data_collator)\neval_dataloader = DataLoader(test_dataset,collate_fn=data_collator,batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:53:48.274436Z","iopub.execute_input":"2024-02-14T07:53:48.275195Z","iopub.status.idle":"2024-02-14T07:53:48.511761Z","shell.execute_reply.started":"2024-02-14T07:53:48.275165Z","shell.execute_reply":"2024-02-14T07:53:48.510859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(),lr=2e-5)\ndevice =  'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:53:48.513236Z","iopub.execute_input":"2024-02-14T07:53:48.513627Z","iopub.status.idle":"2024-02-14T07:53:49.488374Z","shell.execute_reply.started":"2024-02-14T07:53:48.513590Z","shell.execute_reply":"2024-02-14T07:53:49.487426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_epochs = 3\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=1,gamma=0.1)\n\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_train_epochs):\n  #training\n  model.train()\n  total_loss = 0\n  for step, batch in enumerate(train_dataloader):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    logits, _ = model(batch['input_ids'])  # Assuming the model returns logits and presents\n\n    # Calculate the loss manually\n    loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n    loss = loss_fct(logits.view(-1, logits.size(-1)), batch['input_ids'].view(-1))\n\n    # Clear the previous gradients\n    optimizer.zero_grad()\n\n    # Compute gradients\n    loss.backward()\n\n    # Update the model's parameters\n    optimizer.step()\n\n    total_loss += loss.item()\n    progress_bar.update(1)\n\n  average_loss = total_loss / len(train_dataloader)\n  print(f\"Epoch: {epoch+1} Average Loss: {average_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T06:51:12.478313Z","iopub.execute_input":"2024-02-13T06:51:12.479247Z","iopub.status.idle":"2024-02-13T08:21:41.386735Z","shell.execute_reply.started":"2024-02-13T06:51:12.479201Z","shell.execute_reply":"2024-02-13T08:21:41.385503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/nep-gpt/pytorch_model.bin')\nconfig.save_pretrained('/kaggle/working/nep-gpt')","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:53:49.489551Z","iopub.execute_input":"2024-02-14T07:53:49.489907Z","iopub.status.idle":"2024-02-14T07:53:50.277034Z","shell.execute_reply.started":"2024-02-14T07:53:49.489880Z","shell.execute_reply":"2024-02-14T07:53:50.276236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\ngenerate = pipeline(\n    \"text-generation\",\n    model=\"/nep-gpt/\",\n    tokenizer=\"/nep-gpt/\",\n    max_length=150\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:55:16.596514Z","iopub.execute_input":"2024-02-14T07:55:16.596898Z","iopub.status.idle":"2024-02-14T07:55:19.083420Z","shell.execute_reply.started":"2024-02-14T07:55:16.596869Z","shell.execute_reply":"2024-02-14T07:55:19.082305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate('नेपाली भाषामा एउटा वाक्य उत्पन्न गर्नका लागि तपाईंले यो')","metadata":{"execution":{"iopub.status.busy":"2024-02-14T07:55:29.658507Z","iopub.execute_input":"2024-02-14T07:55:29.659277Z","iopub.status.idle":"2024-02-14T07:55:33.590610Z","shell.execute_reply.started":"2024-02-14T07:55:29.659237Z","shell.execute_reply":"2024-02-14T07:55:33.589592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logits.shape ##batch_size,sequence,dimensionality","metadata":{"execution":{"iopub.status.busy":"2024-02-13T08:29:46.466144Z","iopub.execute_input":"2024-02-13T08:29:46.467062Z","iopub.status.idle":"2024-02-13T08:29:46.471219Z","shell.execute_reply.started":"2024-02-13T08:29:46.467027Z","shell.execute_reply":"2024-02-13T08:29:46.470139Z"},"trusted":true},"execution_count":null,"outputs":[]}]}