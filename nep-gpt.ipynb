{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7609791,"sourceType":"datasetVersion","datasetId":4430960}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install git+https://github.com/huggingface/transformers datasets\n!pip install sentencepiece\n!pip install accelerate -U","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-12T07:23:56.257192Z","iopub.execute_input":"2024-02-12T07:23:56.257929Z","iopub.status.idle":"2024-02-12T07:25:14.165460Z","shell.execute_reply.started":"2024-02-12T07:23:56.257896Z","shell.execute_reply":"2024-02-12T07:25:14.164235Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from tokenizers.models import BPE\nfrom tokenizers import Tokenizer\nfrom tokenizers.decoders import ByteLevel as ByteLevelDecoder\nfrom tokenizers.normalizers import NFKC, Sequence\nfrom tokenizers.pre_tokenizers import ByteLevel\nfrom tokenizers.trainers import BpeTrainer\nfrom dataclasses import dataclass\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:33:01.029855Z","iopub.execute_input":"2024-02-12T07:33:01.030483Z","iopub.status.idle":"2024-02-12T07:33:01.075372Z","shell.execute_reply.started":"2024-02-12T07:33:01.030443Z","shell.execute_reply":"2024-02-12T07:33:01.074410Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tokenizers import ByteLevelBPETokenizer\nfrom pathlib import Path\nimport os\n\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train(files='/nepali-wikipedia/output.txt', vocab_size=52_000, min_frequency=2, special_tokens=[\n    \"<s>\",\n    \"<pad>\",\n    \"</s>\",\n    \"<unk>\",\n    \"<mask>\",\n])\n\nsave_path = 'nep-gpt-tokenizer'\nif not os.path.exists(save_path):\n      os.makedirs(save_path)\ntokenizer.save_model(save_path)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:33:05.730918Z","iopub.execute_input":"2024-02-12T07:33:05.731615Z","iopub.status.idle":"2024-02-12T07:33:27.227322Z","shell.execute_reply.started":"2024-02-12T07:33:05.731585Z","shell.execute_reply":"2024-02-12T07:33:27.226302Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['nep-gpt-tokenizer/vocab.json', 'nep-gpt-tokenizer/merges.txt']"},"metadata":{}}]},{"cell_type":"code","source":"import copy\nimport torch\nimport math\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\n\ndef gelu(x):\n  return 0.5*x*(1+torch.tanh(math.sqrt(2/math.pi)*(x+0.044715*torch.pow(x,3))))\n\nclass LayerNorm(nn.Module):\n  def __init__(self, hidden_size, eps=1e-12):\n    super(LayerNorm, self).__init__()\n    self.weight = nn.Parameter(torch.ones(hidden_size))\n    self.bias = nn.Parameter(torch.zeros(hidden_size))\n    self.variance_epsilon = eps\n\n  def forward(self, x):\n    u = x.mean(-1, keepdim=True)\n    s = (x - u).pow(2).mean(-1, keepdim=True)\n    x = (x - u) / torch.sqrt(s + self.variance_epsilon)  # very small value is added to avoid division by zero error if it happens\n    return self.weight * x + self.bias  # w*x+b","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:33:39.112940Z","iopub.execute_input":"2024-02-12T07:33:39.113713Z","iopub.status.idle":"2024-02-12T07:33:42.587170Z","shell.execute_reply.started":"2024-02-12T07:33:39.113680Z","shell.execute_reply":"2024-02-12T07:33:42.586301Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Conv1D(nn.Module):\n  def __init__(self,nf,nx):\n    super(Conv1D,self).__init__()\n    self.nf = nf\n    w = torch.empty(nx,nf)\n    nn.init.normal_(w,std=0.02)\n    self.weight = Parameter(w)\n    self.bias = Parameter(torch.zeros(nf))\n\n  def forward(self,x):\n    size_out = x.size()[:-1] + (self.nf,) # Prepare for matrix multiplication\n    x = torch.addmm(self.bias, x.view(-1,x.size(-1)),self.weight)\n    x = x.view(*size_out)\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:33:42.588789Z","iopub.execute_input":"2024-02-12T07:33:42.589356Z","iopub.status.idle":"2024-02-12T07:33:42.598535Z","shell.execute_reply.started":"2024-02-12T07:33:42.589325Z","shell.execute_reply":"2024-02-12T07:33:42.597474Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx\n        assert n_state % config.n_head == 0\n        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state*3, nx)\n        self.c_proj = Conv1D(n_state, nx)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        nd, ns = w.size(-2), w.size(-1)\n        b = self.bias[:, :, ns - nd:ns, :ns]\n        w = w * b - 1e10 * (1 - b)  # subtract large negative where bias is 0\n        w = nn.Softmax(dim=-1)(w)\n        return torch.matmul(w, v)\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)\n        if k:\n            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n        else:\n            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n\n    def forward(self, x, layer_past=None):\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n\n        if layer_past is not None:\n            past_key, past_value = layer_past[0].transpose(-2,-1), layer_past[1]\n            key = torch.cat((past_value,key), dim=-1)\n            value = torch.cat((past_value,value), dim=-2)\n\n        present = torch.stack((key.transpose(-2,-1), value))  # transpose to have same shape for stacking\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n\n        return a, present\n","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:33:42.600186Z","iopub.execute_input":"2024-02-12T07:33:42.600696Z","iopub.status.idle":"2024-02-12T07:33:42.617170Z","shell.execute_reply.started":"2024-02-12T07:33:42.600663Z","shell.execute_reply":"2024-02-12T07:33:42.616221Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, n_state, config):\n        super(MLP, self).__init__()\n        nx = config.n_embd\n        self.c_fc = Conv1D(n_state, nx)\n        self.c_proj = Conv1D(nx, n_state)\n        self.act = gelu\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return h2\n\n\nclass Block(nn.Module):\n  def __init__(self,n_ctx,config,scale=False):\n    super(Block,self).__init__()\n    nx = config.n_embd\n    self.ln_1 = LayerNorm(nx,eps=config.layer_norm_epsilon)\n    self.attn = Attention(nx,n_ctx,config,scale)\n    self.ln_2 = LayerNorm(nx,eps=config.layer_norm_epsilon)\n    self.mlp = MLP(4*nx,config)\n\n  def forward(self,x,layer_past=None):\n    a,present = self.attn(self.ln_1(x),layer_past=layer_past)\n    x = x+a\n    m = self.mlp(self.ln_2(x))\n    x = x + m\n    return x, present\nclass GPT2Model(nn.Module):\n    def __init__(self, config):\n        super(GPT2Model, self).__init__()\n        self.n_layer = config.n_layer\n        self.n_embd = config.n_embd\n        self.n_vocab = config.vocab_size\n\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n        block = Block(config.n_ctx, config, scale=True)\n        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n\n    def set_embeddings_weights(self, model_embeddings_weights):\n        embed_shape = model_embeddings_weights.shape\n        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n        self.decoder.weight = model_embeddings_weights  # tied weights\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None):\n        if past is None:\n            past_length = 0\n            past = [None] * len(self.h)\n        else:\n            past_length = past[0][0].size(-2)\n\n        if position_ids is None:\n            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        position_ids = position_ids.view(-1, position_ids.size(-1))\n        input_embeds = self.wte(input_ids)\n        position_embeds = self.wpe(position_ids)\n\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n            token_type_embeds = self.wte(token_type_ids)\n        else:\n            token_type_embeds = torch.zeros_like(input_embeds)\n\n        hidden_states = input_embeds + position_embeds + token_type_embeds\n        presents = list()\n\n        for block, layer_past in zip(self.h, past):\n            hidden_states, present = block(hidden_states, layer_past)\n            presents.append(present)\n\n        hidden_states = self.ln_f(hidden_states)\n        output_shape = input_shape + (hidden_states.size(-1),)\n\n        return hidden_states.view(*output_shape), presents","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:33:42.619025Z","iopub.execute_input":"2024-02-12T07:33:42.619393Z","iopub.status.idle":"2024-02-12T07:33:42.640428Z","shell.execute_reply.started":"2024-02-12T07:33:42.619367Z","shell.execute_reply":"2024-02-12T07:33:42.639503Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class GPT2LMHead(nn.Module):\n  def __init__(self, model_embeddings_weights, config):\n    super(GPT2LMHead, self).__init__()\n    self.n_embd = config.n_embd\n    self.set_embeddings_weights(model_embeddings_weights)\n\n  def set_embeddings_weights(self, model_embeddings_weights):\n    embed_shape = model_embeddings_weights.shape\n    self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n    self.decoder.weight = model_embeddings_weights  # tied weights\n\n  def forward(self, hidden_state):\n    lm_logits = self.decoder(hidden_state)\n    return lm_logits\n\nclass GPT2LMHeadModel(nn.Module):\n    def __init__(self, config):\n        super(GPT2LMHeadModel, self).__init__()\n        self.transformer = GPT2Model(config)\n        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n\n    def set_tied(self):\n        \"\"\" Make sure we are sharing the embeddings\n        \"\"\"\n        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        lm_logits = self.lm_head(hidden_states)\n        if lm_labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\n            return loss\n        return lm_logits, presents","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:33:43.205932Z","iopub.execute_input":"2024-02-12T07:33:43.206300Z","iopub.status.idle":"2024-02-12T07:33:43.217821Z","shell.execute_reply.started":"2024-02-12T07:33:43.206272Z","shell.execute_reply":"2024-02-12T07:33:43.216641Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Config, GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained(save_path)\n\ntokenizer.add_special_tokens({\n    \"eos_token\":\"</s>\",\n    \"bos_token\":\"<s>\",\n    \"unk_token\":\"<unk>\",\n    \"pad_token\":\"<pad>\",\n    \"mask_token\":\"<mask>\"\n})\n\nconfig = GPT2Config(\n    vocab_size=tokenizer.vocab_size,\n    bos_token_id=tokenizer.bos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n    n_ctx= 1024\n)\n\nmodel = GPT2LMHeadModel(config)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T09:18:18.591453Z","iopub.execute_input":"2024-02-12T09:18:18.592317Z","iopub.status.idle":"2024-02-12T09:18:18.679585Z","shell.execute_reply.started":"2024-02-12T09:18:18.592279Z","shell.execute_reply":"2024-02-12T09:18:18.678609Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from transformers import LineByLineTextDataset\n\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"/nepali-wikipedia/output.txt\",\n    block_size=128,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:34:19.206107Z","iopub.execute_input":"2024-02-12T07:34:19.206529Z","iopub.status.idle":"2024-02-12T07:37:28.837438Z","shell.execute_reply.started":"2024-02-12T07:34:19.206501Z","shell.execute_reply":"2024-02-12T07:37:28.836226Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\ntrain_dataset,test_dataset = train_test_split(dataset,test_size=0.2)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=False)\n\ntrain_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True,collate_fn=data_collator)\neval_dataloader = DataLoader(test_dataset,collate_fn=data_collator,batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:37:28.840680Z","iopub.execute_input":"2024-02-12T07:37:28.840951Z","iopub.status.idle":"2024-02-12T07:37:29.062661Z","shell.execute_reply.started":"2024-02-12T07:37:28.840928Z","shell.execute_reply":"2024-02-12T07:37:29.061811Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(),lr=2e-5)\ndevice =  'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:38:17.024348Z","iopub.execute_input":"2024-02-12T07:38:17.024769Z","iopub.status.idle":"2024-02-12T07:38:17.965542Z","shell.execute_reply.started":"2024-02-12T07:38:17.024741Z","shell.execute_reply":"2024-02-12T07:38:17.964542Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(49134, 768)\n    (wpe): Embedding(1024, 768)\n    (h): ModuleList(\n      (0-11): 12 x Block(\n        (ln_1): LayerNorm()\n        (attn): Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): GPT2LMHead(\n    (decoder): Linear(in_features=768, out_features=49134, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"num_train_epochs = 1\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=1,gamma=0.1)\n\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_train_epochs):\n  #training\n  model.train()\n  total_loss = 0\n  for step, batch in enumerate(train_dataloader):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    logits, _ = model(batch['input_ids'])  # Assuming the model returns logits and presents\n\n    # Calculate the loss manually\n    loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n    loss = loss_fct(logits.view(-1, logits.size(-1)), batch['input_ids'].view(-1))\n\n    # Clear the previous gradients\n    optimizer.zero_grad()\n\n    # Compute gradients\n    loss.backward()\n\n    # Update the model's parameters\n    optimizer.step()\n\n    total_loss += loss.item()\n    progress_bar.update(1)\n\n  average_loss = total_loss / len(train_dataloader)\n  print(f\"Epoch: {epoch+1} Average Loss: {average_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-12T07:38:21.911154Z","iopub.execute_input":"2024-02-12T07:38:21.911564Z","iopub.status.idle":"2024-02-12T09:08:42.998805Z","shell.execute_reply.started":"2024-02-12T07:38:21.911535Z","shell.execute_reply":"2024-02-12T09:08:42.997875Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c7c296880a4dacaa9041a533cf1eb8"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 Average Loss: 0.0003\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_state_dict.pth')","metadata":{"execution":{"iopub.status.busy":"2024-02-12T09:21:32.989159Z","iopub.execute_input":"2024-02-12T09:21:32.990045Z","iopub.status.idle":"2024-02-12T09:21:33.777762Z","shell.execute_reply.started":"2024-02-12T09:21:32.990013Z","shell.execute_reply":"2024-02-12T09:21:33.776945Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, batch in enumerate(eval_dataloader):\n    # Pass the batch to the model to get the logits\n    logits, _ = model(batch['input_ids'].to(device))\n\n    # Convert the logits to a list of token IDs using greedy decoding\n    token_ids = []\n    for timestep in logits[0]:\n        token_id = torch.argmax(timestep).item()\n        token_ids.append(token_id)\n\n    # Decode the token IDs using the tokenizer\n    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n    print(decoded_text)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T09:26:28.156401Z","iopub.execute_input":"2024-02-12T09:26:28.157007Z","iopub.status.idle":"2024-02-12T09:26:28.804578Z","shell.execute_reply.started":"2024-02-12T09:26:28.156974Z","shell.execute_reply":"2024-02-12T09:26:28.802941Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"प्यालेस्टाइन महिला राष्ट्रिय फुटबल टिम अन्तर्राष्ट्रिय फुटबल प्रतियोगिताहरूमा प्यालेस्टाइनको प्रतिनिधित्व गर्ने महिला फुटबल टिम हो । यो प्यालेस्टाइनमा फुटबलको लागि शासकीय न\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(eval_dataloader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Pass the batch to the model to get the logits\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Convert the logits to a list of token IDs using greedy decoding\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 28\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, position_ids, token_type_ids, lm_labels, past)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lm_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, past\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 28\u001b[0m     hidden_states, presents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lm_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 75\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, position_ids, token_type_ids, past)\u001b[0m\n\u001b[1;32m     72\u001b[0m presents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block, layer_past \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, past):\n\u001b[0;32m---> 75\u001b[0m     hidden_states, present \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     presents\u001b[38;5;241m.\u001b[39mappend(present)\n\u001b[1;32m     78\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, layer_past)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,layer_past\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   a,present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,layer_past\u001b[38;5;241m=\u001b[39mlayer_past)\n\u001b[1;32m     26\u001b[0m   x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m+\u001b[39ma\n\u001b[1;32m     27\u001b[0m   m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m s \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m u)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m u) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(s \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)  \u001b[38;5;66;03m# very small value is added to avoid division by zero error if it happens\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 28.12 MiB is free. Process 2170 has 15.87 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 127.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 28.12 MiB is free. Process 2170 has 15.87 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 127.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"logits.shape ##batch_size,equuence,dimensionaloty","metadata":{"execution":{"iopub.status.busy":"2024-02-12T09:13:06.871174Z","iopub.execute_input":"2024-02-12T09:13:06.872022Z","iopub.status.idle":"2024-02-12T09:13:06.877675Z","shell.execute_reply.started":"2024-02-12T09:13:06.871991Z","shell.execute_reply":"2024-02-12T09:13:06.876701Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"torch.Size([12, 82, 49134])"},"metadata":{}}]}]}